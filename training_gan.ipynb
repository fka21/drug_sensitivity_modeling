{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import os\n",
    "import optuna\n",
    "import scipy\n",
    "from pathlib import Path\n",
    "from shap import Explanation\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, learning_curve, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import uniform, randint\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import ExtraTreesRegressor, BaggingRegressor, HistGradientBoostingRegressor\n",
    "from umap.umap_ import UMAP\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from ctgan import CTGAN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from utils import plot_all_learning_curves, evaluate_models, prefix_params\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Setting up pandas printing options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None) \n",
    "\n",
    "# Setting up working environment\n",
    "base_path = Path.cwd()\n",
    "os.chdir(base_path)\n",
    "\n",
    "# Print out the current working directory\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the provided data\n",
    "print(\"Read in data...\")\n",
    "\n",
    "\n",
    "# Gene expression dataset\n",
    "expr = pd.read_csv(\"input/CCLE_expression.csv\")\n",
    "\n",
    "# Metadata of the samples\n",
    "metadata = pd.read_csv(\"input/sample_info.csv\")\n",
    "\n",
    "#Drug sensitivity data\n",
    "sens = pd.read_excel(\"input/GDSC2_fitted_dose_response_25Feb20.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for downstream analysis\n",
    "print(\"Data wrangling...\")\n",
    "\n",
    "metadata = metadata[metadata['DepMap_ID'].isin(expr['Unnamed: 0'])]\n",
    "metadata = metadata.set_index('DepMap_ID').reindex(expr['Unnamed: 0']).reset_index()\n",
    "\n",
    "# Bring together all tables into singular giant table\n",
    "merged_df = expr.merge(metadata, left_on='Unnamed: 0', right_on='Unnamed: 0').merge(sens[sens['DRUG_NAME'] == 'Lapatinib'], left_on='Sanger_Model_ID', right_on='SANGER_MODEL_ID')\n",
    "\n",
    "# Replace unknown sex with NaN to be imputed\n",
    "merged_df.loc[merged_df[\"sex\"] == \"Unknown\", \"sex\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data to only include the necessary columns and bring the data into the normal scale\n",
    "subset_df = merged_df.iloc[:, 1:(expr.shape)[1]]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(subset_df, merged_df['LN_IC50'], test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of models\n",
    "print(\"Define pipeline and models...\")\n",
    "\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(),\n",
    "    'BaggingRegressor': BaggingRegressor(),\n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor(),\n",
    "    'SVR': SVR(),\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'CatBoostRegressor': CatBoostRegressor(verbose=0)\n",
    "}\n",
    "\n",
    "# Define models and parameter grids\n",
    "model_params = {\n",
    "    'LinearRegression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {'alpha': uniform(0.1, 10)}\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model': Lasso(),\n",
    "        'params': {'alpha': uniform(0.1, 10)}\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'model': ElasticNet(),\n",
    "        'params': {'alpha': uniform(0.1, 10), 'l1_ratio': uniform(0, 1)}\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'model': DecisionTreeRegressor(),\n",
    "        'params': {'max_depth': randint(2, 20), 'min_samples_split': randint(2, 20)}\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'model': RandomForestRegressor(),\n",
    "        'params': {'n_estimators': randint(50, 200), 'max_depth': randint(2, 20)}\n",
    "    },\n",
    "    'ExtraTreesRegressor': {\n",
    "        'model': ExtraTreesRegressor(),\n",
    "        'params': {'n_estimators': randint(50, 200), 'max_depth': randint(2, 20)}\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'model': GradientBoostingRegressor(),\n",
    "        'params': {'n_estimators': randint(50, 200), 'learning_rate': uniform(0.01, 0.3)}\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'model': AdaBoostRegressor(),\n",
    "        'params': {'n_estimators': randint(50, 200), 'learning_rate': uniform(0.01, 1.0)}\n",
    "    },\n",
    "    'BaggingRegressor': {\n",
    "        'model': BaggingRegressor(),\n",
    "        'params': {'n_estimators': randint(10, 100)}\n",
    "    },\n",
    "    'HistGradientBoostingRegressor': {\n",
    "        'model': HistGradientBoostingRegressor(),\n",
    "        'params': {'learning_rate': uniform(0.01, 0.3), 'max_depth': randint(2, 20)}\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model': SVR(),\n",
    "        'params': {'C': uniform(0.1, 10), 'gamma': uniform(0.01, 1)}\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'model': KNeighborsRegressor(),\n",
    "        'params': {'n_neighbors': randint(2, 20), 'weights': ['uniform', 'distance']}\n",
    "    },\n",
    "    'CatBoostRegressor': {\n",
    "        'model': CatBoostRegressor(verbose=0),\n",
    "        'params': {'learning_rate': uniform(0.01, 0.3), 'depth': randint(2, 10)}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['primary_disease', 'sex']\n",
    "numerical_cols = ['age']  # Keep age numeric\n",
    "\n",
    "# One-hot encode categorical features\n",
    "subset_df = merged_df.iloc[:, 1:(expr.shape)[1]]\n",
    "subset_df = pd.concat([subset_df, merged_df[categorical_cols + numerical_cols]], axis=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(subset_df, merged_df['LN_IC50'], test_size=0.2, random_state=17)\n",
    "\n",
    "# Define preprocessing steps\n",
    "# Define preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('variancethreshold', VarianceThreshold(threshold=0.02)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), X_train.columns[:-3]),\n",
    "\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols),  \n",
    "        \n",
    "        ('age', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), ['age'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train GAN...\")\n",
    "\n",
    "# Extract relevant features for GAN training\n",
    "columns_to_synthesize = categorical_cols + numerical_cols + list(X_train.columns[:-3]) # Features to generate\n",
    "discrete_columns = categorical_cols  # Specify categorical columns\n",
    "\n",
    "# Train a CTGAN model\n",
    "ctgan = CTGAN(epochs=150, batch_size=100, verbose=True)\n",
    "ctgan.fit(X_train[columns_to_synthesize], discrete_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic samples\n",
    "num_samples = 500\n",
    "synthetic_data = ctgan.sample(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge real and synthetic data\n",
    "X_train = pd.concat([X_train[columns_to_synthesize], synthetic_data], axis=0)\n",
    "\n",
    "# Ensure target variable has the same number of labels (you can either duplicate y_train or generate labels another way)\n",
    "y_train = pd.concat([y_train, y_train.sample(n=num_samples, replace=True)], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(input_dim, l2_reg=0.01):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Input Layer\n",
    "        layers.InputLayer(input_shape=(input_dim,)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Hidden Layer 1\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Hidden Layer 2\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Hidden Layer 3\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Output Layer\n",
    "        layers.Dense(1)  # Linear activation for regression\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessor on the training data and transform both train and test sets\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Convert to numpy arrays if they're not already\n",
    "X_train_preprocessed = X_train_preprocessed.toarray() if scipy.sparse.issparse(X_train_preprocessed) else X_train_preprocessed\n",
    "X_test_preprocessed = X_test_preprocessed.toarray() if scipy.sparse.issparse(X_test_preprocessed) else X_test_preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compile DL...\", flush=True)\n",
    "\n",
    "# Create the model\n",
    "input_dim = X_train_preprocessed.shape[1]\n",
    "model = create_model(input_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_preprocessed, y_train,\n",
    "    epochs=250,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate DL...\", flush=True)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_mae = model.evaluate(X_test_preprocessed, y_test, verbose=0)\n",
    "print(f\"Test neg_mean_squared_error: {test_mae:.4f}\", flush=True)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_preprocessed)\n",
    "\n",
    "# Calculate R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"R-squared: {r2:.4f}\", flush=True)\n",
    "print(f\"MSE: {mse:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualize DL...\", flush=True)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mse'], label='Training MAE')\n",
    "plt.plot(history.history['val_mse'], label='Validation MAE')\n",
    "plt.title('Model neg_mean_squared_error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/gan_neural_network_learning.pdf\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
